/**
 * @file tokenizer.h
 * @brief POSIX shell tokenizer for recursive descent parsing
 *
 * Provides clean token classification with lookahead support and proper
 * token boundary handling for the recursive descent parser.
 *
 * @author Michael Berry <trismegustis@gmail.com>
 * @copyright Copyright (C) 2021-2026 Michael Berry
 */

#ifndef TOKENIZER_H
#define TOKENIZER_H

#include <stdbool.h>
#include <stddef.h>

// Simple, clean token classification for parser
typedef enum {
    // Basic token types
    TOK_EOF,               // End of input
    TOK_WORD,              // Regular word (command, argument, variable name)
    TOK_STRING,            // Quoted string ('...' - literal)
    TOK_EXPANDABLE_STRING, // Double-quoted string ("...") - needs variable
                           // expansion
    TOK_NUMBER,            // Numeric literal
    TOK_VARIABLE,          // Variable reference ($var, ${var}, etc.)

    // Operators and separators
    TOK_SEMICOLON,        // ;
    TOK_PIPE,             // |
    TOK_AND,              // &
    TOK_LOGICAL_AND,      // &&
    TOK_LOGICAL_OR,       // ||
    TOK_REDIRECT_IN,      // <
    TOK_REDIRECT_OUT,     // >
    TOK_APPEND,           // >>
    TOK_HEREDOC,          // <<
    TOK_HEREDOC_STRIP,    // <<-
    TOK_HERESTRING,       // <<<
    TOK_REDIRECT_ERR,     // 2>
    TOK_REDIRECT_BOTH,    // &>
    TOK_APPEND_ERR,       // 2>>
    TOK_REDIRECT_FD,       // &1, &2, etc.
    TOK_REDIRECT_FD_ALLOC, // {varname}> - fd allocation (bash 4.1+/zsh)
    TOK_REDIRECT_CLOBBER,  // >|
    TOK_ASSIGN,           // =
    TOK_NOT_EQUAL,        // !=
    TOK_PLUS,             // +
    TOK_MINUS,            // -
    TOK_MULTIPLY,         // *
    TOK_DIVIDE,           // /
    TOK_MODULO,           // %
    TOK_GLOB,             // * (when used for globbing)
    TOK_QUESTION,         // ?
    TOK_COMMAND_SUB,      // $(...)
    TOK_ARITH_EXP,        // $((...))
    TOK_BACKQUOTE,        // `

    // Delimiters
    TOK_LPAREN,         // (
    TOK_RPAREN,         // )
    TOK_DOUBLE_LPAREN,  // (( - arithmetic command start
    TOK_DOUBLE_RPAREN,  // )) - arithmetic command end
    TOK_LBRACE,         // {
    TOK_RBRACE,         // }
    TOK_LBRACKET,       // [
    TOK_RBRACKET,       // ]
    TOK_DOUBLE_LBRACKET, // [[ - extended test start
    TOK_DOUBLE_RBRACKET, // ]] - extended test end

    // Extended operators (Phase 1-2)
    TOK_PLUS_ASSIGN,   // += - append to array or add to integer
    TOK_REGEX_MATCH,   // =~ - regex match operator in [[ ]]

    // Process substitution and extended pipes (Phase 3)
    TOK_PROC_SUB_IN,   // <( - process substitution input
    TOK_PROC_SUB_OUT,  // >( - process substitution output
    TOK_PIPE_STDERR,   // |& - pipe both stdout and stderr
    TOK_APPEND_BOTH,   // &>> - append both stdout and stderr

    // Control flow extensions (Phase 5)
    TOK_CASE_FALLTHROUGH, // ;& - case fall-through (execute next without test)
    TOK_CASE_CONTINUE,    // ;;& - case continue (test next pattern)

    // Keywords (recognized contextually)
    TOK_IF,
    TOK_THEN,
    TOK_ELSE,
    TOK_ELIF,
    TOK_FI,
    TOK_WHILE,
    TOK_DO,
    TOK_DONE,
    TOK_FOR,
    TOK_IN,
    TOK_CASE,
    TOK_ESAC,
    TOK_UNTIL,
    TOK_FUNCTION,
    TOK_SELECT,           // select keyword for select loop
    TOK_TIME,             // time keyword for timing pipelines
    TOK_COPROC,           // coproc keyword for coprocesses

    // Special
    TOK_NEWLINE,    // \n (significant in shell)
    TOK_WHITESPACE, // Spaces, tabs (usually ignored)
    TOK_COMMENT,    // # comment
    TOK_ERROR       // Invalid token
} token_type_t;

// Token structure for parser
typedef struct token {
    token_type_t type;
    char *text;         // Token text (null-terminated)
    size_t length;      // Token length
    size_t line;        // Line number (1-based)
    size_t column;      // Column number (1-based)
    size_t position;    // Absolute position in input
    struct token *next; // For token stream
} token_t;

// Tokenizer state for parser
typedef struct tokenizer {
    const char *input;    // Input string
    size_t input_length;  // Input length
    size_t position;      // Current position
    size_t line;          // Current line (1-based)
    size_t column;        // Current column (1-based)
    token_t *current;     // Current token
    token_t *lookahead;   // Next token (for lookahead)
    bool enable_keywords; // Whether to recognize keywords (context-sensitive)
} tokenizer_t;

/* ============================================================================
 * Tokenizer Lifecycle
 * ============================================================================ */

/**
 * @brief Create a new tokenizer for input string
 *
 * @param input Shell command string to tokenize
 * @return New tokenizer instance or NULL on failure
 */
tokenizer_t *tokenizer_new(const char *input);

/**
 * @brief Free a tokenizer and associated resources
 *
 * @param tokenizer Tokenizer to free
 */
void tokenizer_free(tokenizer_t *tokenizer);

/* ============================================================================
 * Token Operations
 * ============================================================================ */

/**
 * @brief Get the current token
 *
 * @param tokenizer Tokenizer context
 * @return Current token or NULL if at end
 */
token_t *tokenizer_current(tokenizer_t *tokenizer);

/**
 * @brief Peek at the next token without consuming
 *
 * @param tokenizer Tokenizer context
 * @return Next token (lookahead) or NULL
 */
token_t *tokenizer_peek(tokenizer_t *tokenizer);

/**
 * @brief Advance to the next token
 *
 * @param tokenizer Tokenizer context
 */
void tokenizer_advance(tokenizer_t *tokenizer);

/**
 * @brief Check if current token matches type
 *
 * @param tokenizer Tokenizer context
 * @param type Token type to match
 * @return True if current token matches type
 */
bool tokenizer_match(tokenizer_t *tokenizer, token_type_t type);

/**
 * @brief Match and consume a token of specified type
 *
 * @param tokenizer Tokenizer context
 * @param type Token type to consume
 * @return True if token was consumed, false otherwise
 */
bool tokenizer_consume(tokenizer_t *tokenizer, token_type_t type);

/* ============================================================================
 * Token Utilities
 * ============================================================================ */

/**
 * @brief Get human-readable name for token type
 *
 * @param type Token type
 * @return String name of token type
 */
const char *token_type_name(token_type_t type);

/**
 * @brief Check if token type is a keyword
 *
 * @param type Token type to check
 * @return True if type is a keyword token
 */
bool token_is_keyword(token_type_t type);

/**
 * @brief Check if token type is an operator
 *
 * @param type Token type to check
 * @return True if type is an operator token
 */
bool token_is_operator(token_type_t type);

/**
 * @brief Check if token type is word-like
 *
 * @param type Token type to check
 * @return True if type represents a word or string
 */
bool token_is_word_like(token_type_t type);

/* ============================================================================
 * Tokenizer Control
 * ============================================================================ */

/**
 * @brief Enable or disable keyword recognition
 *
 * @param tokenizer Tokenizer context
 * @param enable True to recognize keywords, false for all words
 */
void tokenizer_enable_keywords(tokenizer_t *tokenizer, bool enable);

/**
 * @brief Refresh tokenizer state from current position
 *
 * @param tokenizer Tokenizer context
 */
void tokenizer_refresh_from_position(tokenizer_t *tokenizer);

#endif // TOKENIZER_H
